# IO性能篇

## 23 | 基础篇：Linux 文件系统是怎么工作的？

文件系统：文件系统则在磁盘的基础上，提供了一个用来管理文件的树状结构。

索引节点：为了方便管理，Linux 文件系统为每个文件都分配两个数据结构，索引节点（index node）和目录项（directory entry）

- 索引节点，简称为 inode，用来记录文件的元数据。会占用磁盘空间
- 目录项，简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。一个内存数据结构，又称目录项缓存

<img src="https://blog-1300816757.cos.ap-shanghai.myqcloud.com/img/io1.png"/>

注意点：

1. 目录项本身就是一个内存缓存，而索引节点则是存储在磁盘中的数据
2. 磁盘在执行文件系统格式化时，会被分成三个存储区域，超级块、索引节点区和数据块区。
   1. 超级块，存储整个文件系统的状态。
   2. 索引节点区，用来存储索引节点。
   3. 数据块区，则用来存储文件数据。

#### 虚拟文件系统

虚拟文件系统 VFS（Virtual File System），定义了一组所有文件系统都支持的数据结构和标准接口

<img src="https://blog-1300816757.cos.ap-shanghai.myqcloud.com/img/io2.png"/>

#### 文件系统 I/O

VFS 提供了一组标准的文件访问接口。这些接口以系统调用的方式，提供给应用程序使用。

文件读写方式的各种差异，导致 I/O 的分类多种多样

1. 根据是否利用标准库缓存，可以把文件 I/O 分为缓冲 I/O 与非缓冲 I/O。
2. 根据是否利用操作系统的页缓存，可以把文件 I/O 分为直接 I/O 与非直接 I/O。
3. 根据应用程序是否阻塞自身运行，可以把文件 I/O 分为阻塞 I/O 和非阻塞 I/O。
4. 根据是否等待响应结果，可以把文件 I/O 分为同步和异步 I/O。

#### 指标

| 名称           | 获取方式                                            | 解释                       |
| -------------- | --------------------------------------------------- | -------------------------- |
| 磁盘使用空间   | df /dev/sda1                                        |                            |
| 索引节点的容量 | df -i /dev/sda1                                     | Inode 个数                 |
| 页缓存大小     | cat /proc/meminfo \| grep -E "SReclaimable\|Cached" | 也换成                     |
| 缓存类型top    | slabtop                                             | 找到占用内存最多的缓存类型 |

## 24-25 | 基础篇：Linux 磁盘I/O是怎么工作的

概念：磁盘是可以持久化存储的设备，根据介质不同，分为机械磁盘和固态磁盘。

无论机械磁盘，还是固态磁盘，相同磁盘的随机 I/O 都要比连续 I/O 慢很多。

- 对机械磁盘来说，我们刚刚提到过的，由于随机 I/O 需要更多的磁头寻道和盘片旋转，它的性能自然要比连续 I/O 慢。
- 对固态磁盘来说，随机读写会导致大量的垃圾回收，所以相对应的，随机 I/O 的性能比起连续 I/O 来，也还是差了很多。
- 连续 I/O 还可以通过预读的方式，来减少 I/O 请求的次数（优化点）



在 Linux 中，磁盘实际上是作为一个块设备来管理的，也就是以块为单位读写数据，并且支持随机读写。

块：文件系统会把连续的扇区或页，组成逻辑块，然后以逻辑块作为最小单元来管理数据。常见的逻辑块的大小是 4KB，也就是说，连续 8 个扇区，或者单独的一个页，都可以组成一个逻辑块。

#### 通用块层

为了减小不同块设备的差异带来的影响，Linux 通过一个统一的通用块层，来管理各种不同的块设备。

1. 功能跟虚拟文件系统的功能类似。向上，为文件系统和应用程序，提供访问块设备的标准接口；向下，把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序。
2. 通用块层还会给文件系统和应用程序发来的 I/O 请求排队，并通过重新排序、请求合并（io调度）等方式，提高磁盘读写的效率。

Linux 存储系统的 I/O 栈，由上到下分为三个层次，分别是文件系统层、通用块层和设备层。

Linux 通过多种缓存机制来优化 I/O 效率。例如

1. 为了优化文件访问的性能，会使用页缓存、索引节点缓存、目录项缓存等多种缓存机制，以减少对下层块设备的直接调用。
2. 为了优化块设备的访问效率，会使用缓冲区，来缓存块设备的数据。

#### 指标

| 名称                            | 获取方式        | 解释                               |
| ------------------------------- | --------------- | ---------------------------------- |
| 使用率                          | Iostat、pidstat | 磁盘处理 I/O 的时间百分比          |
| 饱和度                          | 综合考虑        | 磁盘处理 I/O 的繁忙程度            |
| IOPS（Input/Output Per Second） | Iostat、pidstat | 每秒的 I/O 请求数                  |
| 吞吐量                          | Iostat、pidstat | 每秒的 I/O 请求大小                |
| 响应时间                        | Iostat、pidstat | I/O 请求从发出到收到响应的间隔时间 |

<img src="https://blog-1300816757.cos.ap-shanghai.myqcloud.com/img/iostat-1.png" style="float:left; width:600px;height:500 px" />

#### 磁盘选型，基准测试：可使用性能工具fio测试

## 26 | 案例篇：如何找出狂打日志的“内鬼”？

pcstat： 查看日志文件在cache中大小

## 案例分析

1. top命令简单观察系统情况，
2. 发现CPU0的iowait较高，发现`python`CPU使用率较高需重点关注
3. 发现内存中，Buffer/Cache 占用内存高达 6GB 之多，这说明内存主要被缓存占用
4. 到这一步，你基本可以判断出，CPU 使用率中的 iowait 是一个潜在瓶颈，而内存部分的缓存占比较大，那磁盘 I/O 又是怎么样的情况呢？
5. 使用`iostat -x -d 1`命令观察
6. 观察 iostat 的最后一列，你会看到，磁盘 sda 的 I/O 使用率已经高达 99%，很可能已经接近 I/O 饱和。
7. 再看前面的各个指标，每秒写磁盘请求数是 64 ，写大小是 32 MB，写请求的响应时间为 7 秒，而请求队列长度则达到了 1100。
8. 断定sda 磁盘已经遇到了严重的性能瓶颈。
9. `pidstat -d 1`观察，发现只有 python 进程的写比较大，而且每秒写的数据超过 45 MB，比上面 iostat 发现的 32MB 的结果还要大。很明显，正是 python 进程导致了 I/O 瓶颈。
10. 在发现kworker，jbd2等内核线程延迟很高，延迟的根源还是大量 I/O。
11. 系统调用相关，通过`strace -p 18940`命令观察
12. `lsof`查看进程打开文件列表，发现
13. 进程打开了文件 /tmp/logtest.txt，并且它的文件描述符是 3 号，而 3 后面的 w ，表示以写的方式打开。
14. 查看源码，最终确认到了问题在于python日志等级不对

## 27 | 案例篇：为什么我的磁盘I/O延迟很高？

#### 案例分析

1. 调用接口，延迟非常高
2. df，这么简单的命令，居然也要等好久才有输出
3. top观察发现，两个 CPU 的 iowait 都非常高，进程部分有一个 python 进程的 CPU 使用率稍微有点高，达到了 14%
4. `iostat -d -x 1`，发现磁盘 sda 的 I/O 使用率已经达到 98% ，接近饱和了。而且，写请求的响应时间高达 18 秒，每秒的写数据为 32 MB，显然写磁盘碰到了瓶颈。
5. `pidstat -d 1`，发现 PID 号为 12280 的结果。这说明，正是案例应用引发 I/O 的性能瓶颈。
6. `strace -p 12280`发现大量的 stat 系统调用，并且大都为 python 的文件，但是，请注意，这里并没有任何 write 系统调用。(ps, 因为都是线程在写，加上`-f`参数可查看)
7. bcc软件包，`./filetop -C`，发现每隔一段时间，线程号为 514 的 python 应用就会先写入大量的 txt 文件，再大量地读。
8. bcc, `opensnoop`发现它打开的文件数量，按照数字编号，从 0.txt 依次增大到 999.txt，这可远多于前面用 filetop 看到的数量。
9. 最终，查看源码后，定位到程序处理算法待优化

## 28 | 案例篇：一个SQL查询要15秒，这是怎么回事？

#### 案例分析

1. `curl`接口发现接口返回的是空数据，而且处理时间超过 15 秒
2. `top`命令观察，两个 CPU 的 iowait 都比较高
3. `iostat -d -x 1`，发现磁盘 sda 每秒的读数据为 32 MB， 而 I/O 使用率高达 97% ，接近饱和，这说明，磁盘 sda 的读取确实碰到了性能瓶颈。
4. `pidstat -d 1`，PID 为 27458 的 mysqld 进程正在进行大量的读，而且读取速度是 32 MB/s，跟刚才 iostat 的发现一致
5. 两个结果一对比，我们自然就找到了磁盘 I/O 瓶颈的根源，即 mysqld 进程。
6. `strace -f -p 27458`，发现线程 28014 正在读取大量数据，且读取文件的描述符编号为 38
7. `pstree -t -a -p 27458`获取父进程，再`lsof -p 27458`，mysqld 进程确实打开了大量文件，而根据文件描述符（FD）的编号，我们知道，描述符为 38 的是一个路径为 /var/lib/mysql/test/products.MYD 的文件。
8. 最终，通过mysql相关命令检查，发现是全表扫描了，通过加索引解决
9. 且mysql读取数据就是文件缓存，dataservice不停的释放文件缓存，就导致MySQL都无法利用磁盘缓存，从而也慢了

## 29 | 案例篇：Redis响应严重延迟，如何解决？

#### 案例分析

1. `curl http://192.168.0.10:10000/get_cache`发现这个接口调用居然要花 10 秒
2. `top`观察发现CPU0 的 iowait 比较高，已经达到了 84%；CPU内存使用率基本正常
3. `iostat -d -x 1`发现，磁盘 sda 每秒的写数据（wkB/s）为 2.5MB，I/O 使用率（%util）是 0。看来，虽然有些 I/O 操作，但并没导致磁盘的 I/O 瓶颈。
4. `pidstat -d 1`发现I/O 最多的进程是 PID 为 9085 的 redis-server，并且它也刚好是在写磁盘。这说明，确实是 redis-server 在进行磁盘写。
5. `strace+lsof`组合，`strace -f -T -tt -p 9085`发现epoll_pwait、read、write、fdatasync 这些系统调用都比较频繁

6. `lsof`发现描述符编号为 3 的是一个 pipe 管道，5 号是 eventpoll，7 号是一个普通文件，而 8 号是一个 TCP socket。
7. 只有 7 号普通文件才会产生磁盘写，而它操作的文件路径是 /data/appendonly.aof，相应的系统调用包括 write 和 fdatasync。
8. 通过redis相关命令排查发现是配置不合理appendfsync 是 always
9. 观察Python 发现应用在查询接口中会调用 Redis 的 SADD 命令，这很可能是不合理使用缓存导致的。

## 30 | 套路篇：如何迅速分析出系统I/O的瓶颈在哪里？

#### 性能指标

<img src="https://blog-1300816757.cos.ap-shanghai.myqcloud.com/img/io3.png"/>

#### 指标 -> 工具

<img src="https://blog-1300816757.cos.ap-shanghai.myqcloud.com/img/io4.png" style="float:left; width:600px;height:500 px" />

#### 工具->指标

<img src="https://blog-1300816757.cos.ap-shanghai.myqcloud.com/img/io5.png" style="float:left; width:600px;height:500 px" />

#### 如何迅速分析 I/O 的性能瓶颈

还是那句话，**找关联**。多种性能指标间都有一定的关联性，不要完全孤立的看待他们，想弄清楚性能指标的关联性，就要**通晓每种性能指标的工作原理**

#### 排查思路

<img src="https://blog-1300816757.cos.ap-shanghai.myqcloud.com/img/io6.png"/>

## 31 | 套路篇：磁盘 I/O 性能优化的几个思路

#### I/O基准测试

fio可单独测试，配合blktrace记录磁盘编写，并用fio进行重放

#### 应用程序优化

1. 可以用追加写代替随机写，减少寻址开销，加快 I/O 写的速度。
2. 可以借助缓存 I/O ，充分利用系统缓存，降低实际 I/O 的次数。
3. 可以在应用程序内部构建自己的缓存，或者用 Redis 这类外部缓存系统。
4. 在需要频繁读写同一块磁盘空间时，可以用 mmap 代替 read/write，减少内存的拷贝次数。
5. 在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写入磁盘，即可以用 fsync() 取代 O_SYNC。
6. 多个应用程序共享相同磁盘时，为了保证 I/O 不被某个应用完全占用，推荐你使用 cgroups 的 I/O 子系统，来限制进程 / 进程组的 IOPS 以及吞吐量。
7. 在使用 CFQ 调度器时，可以用 ionice 来调整进程的 I/O 调度优先级，特别是提高核心应用的 I/O 优先级。

#### 文件系统优化

1. 根据实际负载场景的不同，选择最适合的文件系统。
2. 优化文件系统的配置选项，包括文件系统的特性（如 ext_attr、dir_index）、日志模式（如 journal、ordered、writeback）、挂载选项（如 noatime）等等。
3. 优化文件系统的缓存。
   1. 优化 pdflush 脏页
   2. 优化内核回收目录项缓存和索引节点缓存的倾向
   3. 不需要持久化时，你还可以用内存文件系统 tmpfs，以获得更好的 I/O 性能 。

#### 磁盘优化

1. 换用性能更好的磁盘
2. 使用 RAID ，把多块磁盘组合成一个逻辑磁盘
3. 可以对应用程序的数据，进行磁盘级别的隔离。如日志、数据等
4. 顺序读比较多的场景中，我们可以增大磁盘的预读数据
5. 优化内核块设备 I/O 的选项，调整磁盘队列的长度 /sys/block/sdb/queue/nr_requests，适当增大队列长度，可以提升磁盘的吞吐量
6. 磁盘本身出现硬件错误， dmesg查看排查日志